import org.apache.spark.sql.{SparkSession}
import org.apache.spark.sql.functions._        



// Create Spark Session

val spark = SparkSession
  .builder()
  .master("local")
  .appName("Rate Source")
  .getOrCreate()



// specified the format as rate and specified rowsPerSecond = 1 to generate 1 row for each micro-batch and load the data into initDF streaming DataFrame.
  

  val initDF = (spark
  .readStream
  .format("rate")
  .option("rowsPerSecond", 1)
  .load()
  )

println("Streaming DataFrame : " + initDF.isStreaming)



basic transformation on initDF to generate another column result by just adding 1 to column value

val resultDF = initDF
    .withColumn("result", col("value") + lit(1))


initDF
  .writeStream
  .outputMode("append")
  .option("truncate", false)
  .format("console")
  .start()
  .awaitTermination()